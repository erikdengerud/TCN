/var/spool/slurmd/job730435/slurm_script: line 12: $'\302\240': command not found
train_start          : 2012-01-01
train_end            : 2014-12-16
v_batch_size         : 32
h_batch_size         : 256
num_workers          : 4
time_covariates      : True
one_hot_id           : False
num_layers           : 5
kernel_size          : 7
res_block_size       : 32
bias                 : True
epochs               : 500
lr                   : 0.0005
dropout              : 0.0
stride               : 1
leveledinit          : True
clip                 : False
num_rolling_periods  : 7
length_rolling       : 24
model_save_path      : electricity_dglo_data/models/tcn_dglo.pt
writer_path          : electricity_dglo_data/runs/tcn_dglo
log_interval         : 5
print                : False
cuda:0
Creating dataset.
Receptive field of the model is 385 time points.
Train dataset
Dimension of X :  torch.Size([370, 8, 25944])
Dimension of Y :  torch.Size([370, 1, 25944])
Test dataset
Dimension of X :  torch.Size([370, 8, 576])
Dimension of Y :  torch.Size([370, 1, 576])
25944
576
Number of learnable parameters : 67508
Traceback (most recent call last):
  File "electricity_dglo_data/run_electricity.py", line 187, in <module>
    optimizer = optim.Adam(tcn.parameters(), lr=args.lr)
  File "/share/apps/software/PyTorch/1.3.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/torch/optim/adam.py", line 42, in __init__
    super(Adam, self).__init__(params, defaults)
  File "/share/apps/software/PyTorch/1.3.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/torch/optim/optimizer.py", line 51, in __init__
    self.add_param_group(param_group)
  File "/share/apps/software/PyTorch/1.3.1-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/torch/optim/optimizer.py", line 202, in add_param_group
    raise ValueError("can't optimize a non-leaf Tensor")
ValueError: can't optimize a non-leaf Tensor
